{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание №14 по теме \"Transfer learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7HTdlC4FSfh",
    "outputId": "8430e101-23ed-4aaf-9645-3bfadd41eaff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.22.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFZUMgNDpQAV",
    "outputId": "0179fc55-9e1e-41dd-a695-4151ad48ba6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rfk03\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cGH2aI9CGgbp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KzzQU7MgFSh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfk03\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:211: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from transformers.trainer import logger as noisy_logger\n",
    "noisy_logger.setLevel(logging.WARNING)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cOb1UySVFSkR"
   },
   "outputs": [],
   "source": [
    "model_name = 'ai-forever/rugpt3small_based_on_gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L9Fbgd5TFSm0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('C:\\Study\\dataset.jsonl', lines=True).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "WViaQk6lFSpF",
    "outputId": "d69d59ea-cc0f-485a-d140-1d2296490e77"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-30 11:24:00+00:00</td>\n",
       "      <td>22010.0</td>\n",
       "      <td>&lt;Ares&gt; ppdv, все юниксы очень дружелюбны.. они...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-30 11:25:00+00:00</td>\n",
       "      <td>25105.0</td>\n",
       "      <td>&lt;томатик_рад&gt; а ты не чувствуешь красоту мира?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-30 11:27:00+00:00</td>\n",
       "      <td>7192.0</td>\n",
       "      <td>&lt;Дор&gt; \"мышка, почему у тебя такие большие глаз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-30 11:28:00+00:00</td>\n",
       "      <td>29169.0</td>\n",
       "      <td>&lt;PPDV[os2]&gt; \"Мальчики, вы что больные, бегать ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004-08-30 11:26:00+00:00</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>&lt;Ohtori_Akio&gt; мы - как разработчики - живём с ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date   rating  \\\n",
       "id                                      \n",
       "1  2004-08-30 11:24:00+00:00  22010.0   \n",
       "2  2004-08-30 11:25:00+00:00  25105.0   \n",
       "3  2004-08-30 11:27:00+00:00   7192.0   \n",
       "4  2004-08-30 11:28:00+00:00  29169.0   \n",
       "5  2004-08-30 11:26:00+00:00   7140.0   \n",
       "\n",
       "                                                 text  \n",
       "id                                                     \n",
       "1   <Ares> ppdv, все юниксы очень дружелюбны.. они...  \n",
       "2   <томатик_рад> а ты не чувствуешь красоту мира?...  \n",
       "3   <Дор> \"мышка, почему у тебя такие большие глаз...  \n",
       "4   <PPDV[os2]> \"Мальчики, вы что больные, бегать ...  \n",
       "5   <Ohtori_Akio> мы - как разработчики - живём с ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mVb2rTggFSrc"
   },
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "  clr_text = re.sub(r\"<.*?>\", \" \",text).lower()\n",
    "  clr_text = summary = re.sub(r\"\\s\", \" \", clr_text)\n",
    "  return clr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "VVASTO_-FSt7",
    "outputId": "737e9e7b-5f85-4602-8733-1f254097332c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>clear_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-30 11:24:00+00:00</td>\n",
       "      <td>22010.0</td>\n",
       "      <td>&lt;Ares&gt; ppdv, все юниксы очень дружелюбны.. они...</td>\n",
       "      <td>ppdv, все юниксы очень дружелюбны.. они прос...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-30 11:25:00+00:00</td>\n",
       "      <td>25105.0</td>\n",
       "      <td>&lt;томатик_рад&gt; а ты не чувствуешь красоту мира?...</td>\n",
       "      <td>а ты не чувствуешь красоту мира?   честно го...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-30 11:27:00+00:00</td>\n",
       "      <td>7192.0</td>\n",
       "      <td>&lt;Дор&gt; \"мышка, почему у тебя такие большие глаз...</td>\n",
       "      <td>\"мышка, почему у тебя такие большие глаза?\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-30 11:28:00+00:00</td>\n",
       "      <td>29169.0</td>\n",
       "      <td>&lt;PPDV[os2]&gt; \"Мальчики, вы что больные, бегать ...</td>\n",
       "      <td>\"мальчики, вы что больные, бегать в палату к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2004-08-30 11:26:00+00:00</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>&lt;Ohtori_Akio&gt; мы - как разработчики - живём с ...</td>\n",
       "      <td>мы - как разработчики - живём с субейзом под...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date   rating  \\\n",
       "id                                      \n",
       "1  2004-08-30 11:24:00+00:00  22010.0   \n",
       "2  2004-08-30 11:25:00+00:00  25105.0   \n",
       "3  2004-08-30 11:27:00+00:00   7192.0   \n",
       "4  2004-08-30 11:28:00+00:00  29169.0   \n",
       "5  2004-08-30 11:26:00+00:00   7140.0   \n",
       "\n",
       "                                                 text  \\\n",
       "id                                                      \n",
       "1   <Ares> ppdv, все юниксы очень дружелюбны.. они...   \n",
       "2   <томатик_рад> а ты не чувствуешь красоту мира?...   \n",
       "3   <Дор> \"мышка, почему у тебя такие большие глаз...   \n",
       "4   <PPDV[os2]> \"Мальчики, вы что больные, бегать ...   \n",
       "5   <Ohtori_Akio> мы - как разработчики - живём с ...   \n",
       "\n",
       "                                           clear_text  \n",
       "id                                                     \n",
       "1     ppdv, все юниксы очень дружелюбны.. они прос...  \n",
       "2     а ты не чувствуешь красоту мира?   честно го...  \n",
       "3     \"мышка, почему у тебя такие большие глаза?\" ...  \n",
       "4     \"мальчики, вы что больные, бегать в палату к...  \n",
       "5     мы - как разработчики - живём с субейзом под...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clear_text'] = df['text'].apply(lambda x: clear_text(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzDaqqwMFSwU",
    "outputId": "41b9dcfd-ed63-4735-fd4f-ba15d20be063"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1           ppdv, все юниксы очень дружелюбны.. они прос...\n",
       "2           а ты не чувствуешь красоту мира?   честно го...\n",
       "3           \"мышка, почему у тебя такие большие глаза?\" ...\n",
       "4           \"мальчики, вы что больные, бегать в палату к...\n",
       "5           мы - как разработчики - живём с субейзом под...\n",
       "                                ...                        \n",
       "463644    xxx: угадайте не гугля, что такое жопиздан! yy...\n",
       "463645    xxx: посетила шальная мысль заняться собой, жи...\n",
       "463646    #всебожьяроса xxx: судьба айтишников вообще не...\n",
       "463647    прибывшие на место правоохранители установили,...\n",
       "463648    ххх: вот ребята из английской фирмы tvr (давны...\n",
       "Name: clear_text, Length: 81497, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.loc[:,'clear_text']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tzZwRV7wH7u9"
   },
   "outputs": [],
   "source": [
    "def build_text_files(data_json, dest_path):\n",
    "  f = open(dest_path, 'w', encoding=\"utf-8\")\n",
    "  data = ''\n",
    "  for texts in data_json:\n",
    "    summary = str(texts).strip()\n",
    "    data += summary + \" \"\n",
    "  f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bVUK14dVH7xN"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.2)\n",
    "\n",
    "build_text_files(train, 'C:/Study/train_dataset.txt')\n",
    "build_text_files(test, 'C:/Study/test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnzG_J9xH7zR",
    "outputId": "a87be85d-0e67-44f4-ebee-1e45cb1771ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ai-forever/rugpt3small_based_on_gpt2')\n",
    "\n",
    "train_path = 'C:/Study/train_dataset.txt'\n",
    "test_path = 'C:/Study/test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "63IAXyMlKrbR"
   },
   "outputs": [],
   "source": [
    "from transformers.data import data_collator\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "  train_dataset = TextDataset(\n",
    "      tokenizer = tokenizer,\n",
    "      file_path = train_path,\n",
    "      block_size = 128)\n",
    "  \n",
    "  test_dataset = TextDataset(\n",
    "      tokenizer = tokenizer,\n",
    "      file_path = test_path,\n",
    "      block_size = 128)\n",
    "  \n",
    "  data_collator = DataCollatorForLanguageModeling(\n",
    "      tokenizer = tokenizer, mlm = False)\n",
    "  \n",
    "  return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9bCfyBQVKrdd"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ATVfhdTpKrfj"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    'phrase',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 1,\n",
    "    learning_rate = 0.001,\n",
    "    weight_decay = 0.01,\n",
    "    save_strategy = 'no',\n",
    "    report_to = 'none')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "OH-wsq1NKrhl",
    "outputId": "9c2732d6-8f8b-4a3e-bd7a-14586f994e7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4177' max='4176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4176/4176 7:12:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1038' max='1038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1038/1038 36:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2034\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2034\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2037\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2300\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2298\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2300\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2303\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3029\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3026\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3028\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3029\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3030\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3032\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3033\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3039\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:3271\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;66;03m# Gather all remaining tensors and put them back on the CPU\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m losses_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3271\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mnested_numpify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3272\u001b[0m     all_losses \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;28;01mif\u001b[39;00m all_losses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate((all_losses, losses), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   3273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer_pt_utils.py:160\u001b[0m, in \u001b[0;36mnested_numpify\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# As of Numpy 1.21.4, NumPy does not support bfloat16 (see\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# Until Numpy adds bfloat16, we must convert float32.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-J5TQYmFSyh"
   },
   "outputs": [],
   "source": [
    "def generate_text(prefix):\n",
    "    tokens = tokenizer(prefix, ruturn_tensors='pt').to('cuda')\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "    \n",
    "    output = model.generate(\n",
    "        **tokens,\n",
    "        do_sample = False,\n",
    "        max_length = size+50,\n",
    "        early_stopping = True,\n",
    "        length_penalty=2.0,\n",
    "        repetiton_penalty=8.,\n",
    "        temperature - 0.5,\n",
    "        num_beams = 3,\n",
    "        no_repeat_ngram_size=5\n",
    "    )\n",
    "    \n",
    "    decoded = tokenizer.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    return prefix + result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text('На фоне тьмы ввысь белым бил фонтан'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
