{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание №10 по теме \"Модель seq2seq и механизм внимания\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7Z2txZfv6hzq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U83cAba96k91"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"C:/Study/fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I4OfbZl76lCa"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "  w = w.strip()\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "dRnvEvunBTXG",
    "outputId": "48239af0-b69e-4620-a413-fd89f0490423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "08U3p_o4BTZU"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgXggpjS6lEz",
    "outputId": "5476e116-4996-427e-ea6a-39911c37fc82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go . <end>\n",
      "<start> va ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, fra = create_dataset(path_to_file, None)\n",
    "print(en[0])\n",
    "print(fra[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lWlIFwle6lHV"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "  \n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vvy3iMFU6lJa"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IKjWaMcC4Oo",
    "outputId": "b1329384-8fa2-4ddd-9701-07479002f9ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217975, 217975)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SKJXQfjZC4RJ"
   },
   "outputs": [],
   "source": [
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_5kasjTC4Th",
    "outputId": "8c7edca6-e46c-4a79-893a-f839d4e05e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "k8pjAkdPC4Vx"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63r6kflGC4X9",
    "outputId": "9a1d0439-feac-4bdd-d3cf-6682d6805f82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 19]), TensorShape([64, 12]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QteaN-IRDXbs"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8ULj0uRADXeR"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "d9Wr0IZ8DXgn"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OvWN4kV-DXi9"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_JRX-Jk2DXlK"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "x0zANZfXC4aN"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'C:/Study/'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fV1hdN2_Epfb"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "DobO4fm6Ephx",
    "outputId": "9b8d6e0a-85d0-4c55-8348-d7f25fdd0ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7935\n",
      "Epoch 1 Batch 100 Loss 2.0981\n",
      "Epoch 1 Batch 200 Loss 2.1184\n",
      "Epoch 1 Batch 300 Loss 1.8051\n",
      "Epoch 1 Batch 400 Loss 1.7206\n",
      "Epoch 1 Batch 500 Loss 1.4904\n",
      "Epoch 1 Batch 600 Loss 1.5101\n",
      "Epoch 1 Batch 700 Loss 1.3693\n",
      "Epoch 1 Batch 800 Loss 1.4130\n",
      "Epoch 1 Batch 900 Loss 1.2496\n",
      "Epoch 1 Batch 1000 Loss 1.3192\n",
      "Epoch 1 Batch 1100 Loss 1.2086\n",
      "Epoch 1 Batch 1200 Loss 1.2471\n",
      "Epoch 1 Loss 1.6127\n",
      "Time taken for 1 epoch 1116.0044150352478 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0750\n",
      "Epoch 2 Batch 100 Loss 0.9925\n",
      "Epoch 2 Batch 200 Loss 1.0780\n",
      "Epoch 2 Batch 300 Loss 0.9379\n",
      "Epoch 2 Batch 400 Loss 0.9990\n",
      "Epoch 2 Batch 500 Loss 0.8004\n",
      "Epoch 2 Batch 600 Loss 0.8726\n",
      "Epoch 2 Batch 700 Loss 0.7348\n",
      "Epoch 2 Batch 800 Loss 0.8886\n",
      "Epoch 2 Batch 900 Loss 0.7157\n",
      "Epoch 2 Batch 1000 Loss 0.7462\n",
      "Epoch 2 Batch 1100 Loss 0.6586\n",
      "Epoch 2 Batch 1200 Loss 0.6523\n",
      "Epoch 2 Loss 0.8498\n",
      "Time taken for 1 epoch 1132.9146301746368 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5163\n",
      "Epoch 3 Batch 100 Loss 0.5068\n",
      "Epoch 3 Batch 200 Loss 0.5080\n",
      "Epoch 3 Batch 300 Loss 0.5898\n",
      "Epoch 3 Batch 400 Loss 0.5170\n",
      "Epoch 3 Batch 500 Loss 0.4833\n",
      "Epoch 3 Batch 600 Loss 0.5317\n",
      "Epoch 3 Batch 700 Loss 0.4619\n",
      "Epoch 3 Batch 800 Loss 0.4474\n",
      "Epoch 3 Batch 900 Loss 0.4663\n",
      "Epoch 3 Batch 1000 Loss 0.4957\n",
      "Epoch 3 Batch 1100 Loss 0.4539\n",
      "Epoch 3 Batch 1200 Loss 0.5121\n",
      "Epoch 3 Loss 0.4811\n",
      "Time taken for 1 epoch 1133.1338028907776 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3307\n",
      "Epoch 4 Batch 100 Loss 0.2624\n",
      "Epoch 4 Batch 200 Loss 0.2795\n",
      "Epoch 4 Batch 300 Loss 0.3317\n",
      "Epoch 4 Batch 400 Loss 0.2943\n",
      "Epoch 4 Batch 500 Loss 0.2488\n",
      "Epoch 4 Batch 600 Loss 0.2700\n",
      "Epoch 4 Batch 700 Loss 0.2695\n",
      "Epoch 4 Batch 800 Loss 0.2805\n",
      "Epoch 4 Batch 900 Loss 0.3831\n",
      "Epoch 4 Batch 1000 Loss 0.3086\n",
      "Epoch 4 Batch 1100 Loss 0.2852\n",
      "Epoch 4 Batch 1200 Loss 0.3121\n",
      "Epoch 4 Loss 0.2911\n",
      "Time taken for 1 epoch 1138.4229037761688 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1665\n",
      "Epoch 5 Batch 100 Loss 0.1373\n",
      "Epoch 5 Batch 200 Loss 0.1889\n",
      "Epoch 5 Batch 300 Loss 0.2492\n",
      "Epoch 5 Batch 400 Loss 0.2012\n",
      "Epoch 5 Batch 500 Loss 0.1907\n",
      "Epoch 5 Batch 600 Loss 0.1956\n",
      "Epoch 5 Batch 700 Loss 0.1945\n",
      "Epoch 5 Batch 800 Loss 0.1987\n",
      "Epoch 5 Batch 900 Loss 0.1465\n",
      "Epoch 5 Batch 1000 Loss 0.1838\n",
      "Epoch 5 Batch 1100 Loss 0.2435\n",
      "Epoch 5 Batch 1200 Loss 0.2686\n",
      "Epoch 5 Loss 0.1950\n",
      "Time taken for 1 epoch 1138.3342168331146 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1400\n",
      "Epoch 6 Batch 100 Loss 0.1140\n",
      "Epoch 6 Batch 200 Loss 0.1160\n",
      "Epoch 6 Batch 300 Loss 0.1390\n",
      "Epoch 6 Batch 400 Loss 0.1411\n",
      "Epoch 6 Batch 500 Loss 0.1435\n",
      "Epoch 6 Batch 600 Loss 0.1449\n",
      "Epoch 6 Batch 700 Loss 0.1616\n",
      "Epoch 6 Batch 800 Loss 0.1319\n",
      "Epoch 6 Batch 900 Loss 0.1486\n",
      "Epoch 6 Batch 1000 Loss 0.1627\n",
      "Epoch 6 Batch 1100 Loss 0.1500\n",
      "Epoch 6 Batch 1200 Loss 0.1844\n",
      "Epoch 6 Loss 0.1471\n",
      "Time taken for 1 epoch 1143.8610887527466 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0787\n",
      "Epoch 7 Batch 100 Loss 0.1140\n",
      "Epoch 7 Batch 200 Loss 0.1010\n",
      "Epoch 7 Batch 300 Loss 0.0958\n",
      "Epoch 7 Batch 400 Loss 0.1224\n",
      "Epoch 7 Batch 500 Loss 0.1187\n",
      "Epoch 7 Batch 600 Loss 0.0842\n",
      "Epoch 7 Batch 700 Loss 0.1685\n",
      "Epoch 7 Batch 800 Loss 0.1521\n",
      "Epoch 7 Batch 900 Loss 0.1230\n",
      "Epoch 7 Batch 1000 Loss 0.1452\n",
      "Epoch 7 Batch 1100 Loss 0.1804\n",
      "Epoch 7 Batch 1200 Loss 0.2110\n",
      "Epoch 7 Loss 0.1215\n",
      "Time taken for 1 epoch 1143.58154463768 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1052\n",
      "Epoch 8 Batch 100 Loss 0.1043\n",
      "Epoch 8 Batch 200 Loss 0.0696\n",
      "Epoch 8 Batch 300 Loss 0.0826\n",
      "Epoch 8 Batch 400 Loss 0.0805\n",
      "Epoch 8 Batch 500 Loss 0.1079\n",
      "Epoch 8 Batch 600 Loss 0.0937\n",
      "Epoch 8 Batch 700 Loss 0.1374\n",
      "Epoch 8 Batch 800 Loss 0.1302\n",
      "Epoch 8 Batch 900 Loss 0.1031\n",
      "Epoch 8 Batch 1000 Loss 0.1214\n",
      "Epoch 8 Batch 1100 Loss 0.1280\n",
      "Epoch 8 Batch 1200 Loss 0.1070\n",
      "Epoch 8 Loss 0.1098\n",
      "Time taken for 1 epoch 1148.7844944000244 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0614\n",
      "Epoch 9 Batch 100 Loss 0.0939\n",
      "Epoch 9 Batch 200 Loss 0.0797\n",
      "Epoch 9 Batch 300 Loss 0.0791\n",
      "Epoch 9 Batch 400 Loss 0.0743\n",
      "Epoch 9 Batch 500 Loss 0.1181\n",
      "Epoch 9 Batch 600 Loss 0.0873\n",
      "Epoch 9 Batch 700 Loss 0.0853\n",
      "Epoch 9 Batch 800 Loss 0.0826\n",
      "Epoch 9 Batch 900 Loss 0.0938\n",
      "Epoch 9 Batch 1000 Loss 0.1229\n",
      "Epoch 9 Batch 1100 Loss 0.1162\n",
      "Epoch 9 Batch 1200 Loss 0.1083\n",
      "Epoch 9 Loss 0.1007\n",
      "Time taken for 1 epoch 1146.8819642066956 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1139\n",
      "Epoch 10 Batch 100 Loss 0.0494\n",
      "Epoch 10 Batch 200 Loss 0.0719\n",
      "Epoch 10 Batch 300 Loss 0.0691\n",
      "Epoch 10 Batch 400 Loss 0.1176\n",
      "Epoch 10 Batch 500 Loss 0.0793\n",
      "Epoch 10 Batch 600 Loss 0.0804\n",
      "Epoch 10 Batch 700 Loss 0.0961\n",
      "Epoch 10 Batch 800 Loss 0.0922\n",
      "Epoch 10 Batch 900 Loss 0.1110\n",
      "Epoch 10 Batch 1000 Loss 0.0966\n",
      "Epoch 10 Batch 1100 Loss 0.1151\n",
      "Epoch 10 Batch 1200 Loss 0.0864\n",
      "Epoch 10 Loss 0.0950\n",
      "Time taken for 1 epoch 1154.0081436634064 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TVH6Pcx_EpkC"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "23tiVNHCEpmZ"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3UuDTqjmGEfW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x19877955e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mr5Mr0PIGEh8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> je vois tous les gens du monde . <end>\n",
      "Predicted translation: i eat all about them . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"Je vois tous les gens du monde.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ils voulaient voyager . <end>\n",
      "Predicted translation: they wanted to travel . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"Ils voulaient voyager.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> voler vers le ciel est notre espoir . <end>\n",
      "Predicted translation: we must be caught . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"Voler vers le ciel est notre espoir.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
